{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ccd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98b3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "b1 = unpickle('data_batch_1')\n",
    "b2 = unpickle('data_batch_2')\n",
    "b3 = unpickle('data_batch_3')\n",
    "b4 = unpickle('data_batch_4')\n",
    "b5 = unpickle('data_batch_5')\n",
    "bt = unpickle('test_batch')\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25b843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = bt[b'data'].reshape(len(bt[b'data']),3,32,32).transpose(0,2,3,1)\n",
    "y_t = bt[b'labels']\n",
    "y_t = np.array(y_t).reshape((len(bt[b'data']),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3421d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_t[27])\n",
    "print(labels[y_t[27]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9d9e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_1 = b1[b'data'].reshape(len(b1[b'data']),3,32,32).transpose(0,2,3,1)\n",
    "y_1 = b1[b'labels']\n",
    "X_2 = b2[b'data'].reshape(len(b1[b'data']),3,32,32).transpose(0,2,3,1)\n",
    "y_2 = b2[b'labels']\n",
    "X_3 = b3[b'data'].reshape(len(b1[b'data']),3,32,32).transpose(0,2,3,1)\n",
    "y_3 = b3[b'labels']\n",
    "X_4 = b4[b'data'].reshape(len(b1[b'data']),3,32,32).transpose(0,2,3,1)\n",
    "y_4 = b4[b'labels']\n",
    "X_5 = b5[b'data'].reshape(len(b1[b'data']),3,32,32).transpose(0,2,3,1)\n",
    "y_5 = b5[b'labels']\n",
    "#Xs = [X_1,X_2,X_3,X_4,X_5]\n",
    "X_ = np.stack([X_1,X_2,X_3,X_4,X_5],axis=0).reshape((50000,32,32,3))\n",
    "ys = [np.array(y_1).reshape((10000,1)),np.array(y_2).reshape((10000,1)),np.array(y_3).reshape((10000,1)),np.array(y_4).reshape((10000,1)),np.array(y_5).reshape((10000,1))]\n",
    "y_train = np.stack(ys,axis=0).reshape((50000,))\n",
    "\n",
    "\n",
    "plt.imshow(X_[27])\n",
    "print(labels[y_train[27]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0155f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23ddf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2bw(img):\n",
    "    r,g,b = img[:,:,0],img[:,:,1],img[:,:,2]\n",
    "    #return  0.5*r + 0.7*g + 0.1*b\n",
    "    return 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "X_train = np.array([rgb2bw(img) for img in X_])\n",
    "X_test = np.array([rgb2bw(img) for img in X_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bcab30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self,num_filters,kernel_size = 3,padding=0, strides=(1,1)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.filters = np.random.randn(num_filters, self.kernel_size, self.kernel_size) / 9\n",
    "        self.padding = padding\n",
    "        self.strides = strides\n",
    "        \n",
    "        \n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible (self.kernel_size x self.kernel_size) image regions.\n",
    "        - image is a 2d numpy array.\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - self.kernel_size-1):\n",
    "            for j in range(w - self.kernel_size-1):\n",
    "                im_region = image[i:(i + self.kernel_size), j:(j + self.kernel_size)]\n",
    "                yield im_region, i, j\n",
    "    \n",
    "    def forward(self, img):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input image.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        if self.padding != 0:\n",
    "            imagepadded = np.zeros((img.shape[0] + self.padding*2, img.shape[1] + self.padding*2))\n",
    "            imagepadded[int(self.padding):int(-1 * self.padding), int(self.padding):int(-1 * self.padding)] = img\n",
    "        else:\n",
    "            imagepadded = img\n",
    "        \n",
    "        self.last_input = imagepadded\n",
    "\n",
    "        h, w = imagepadded.shape\n",
    "        output = np.zeros(((h - self.kernel_size-1), (w - self.kernel_size-1), len(self.filters)))\n",
    "        \n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(imagepadded):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a hyperparameter.\n",
    "        '''\n",
    "        d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(len(self.filters)):\n",
    "                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "        # Update filters\n",
    "        self.filters -= learn_rate * d_L_d_filters\n",
    "\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee06f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2:\n",
    "    def __init__(self,kernel_size = 3,padding=0, strides=(1,1)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.strides = strides\n",
    "    \n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping (kernel_size x kernel_size) image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, img):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h // 2, w // 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        if self.padding != 0:\n",
    "            imagepadded = np.zeros((img.shape[0] + self.padding*2, img.shape[1] + self.padding*2))\n",
    "            imagepadded[int(self.padding):int(-1 * self.padding), int(self.padding):int(-1 * self.padding)] = img\n",
    "        else:\n",
    "            imagepadded = img\n",
    "            \n",
    "        self.last_input = imagepadded\n",
    "\n",
    "        h, w, num_filters = imagepadded.shape\n",
    "        \n",
    "        output = np.zeros((h // (self.kernel_size-1), w // (self.kernel_size-1), num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(imagepadded):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                # If this pixel was the max value, copy the gradient to it.\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e68ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    # A standard fully-connected layer with softmax activation.\n",
    "    def __init__(self, input_len, nodes):\n",
    "        # We divide by input_len to reduce the variance of our initial values\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        '''\n",
    "        Performs a forward pass of the softmax layer using the given input.\n",
    "        Returns a 1d numpy array containing the respective probability values.\n",
    "        - input can be any array with any dimensions.\n",
    "        '''\n",
    "        self.last_input_shape = inp.shape #14x14x16\n",
    "        #print(inp.shape)\n",
    "        \n",
    "        inp = inp.flatten()\n",
    "        #print(inp.shape)\n",
    "        \n",
    "        self.last_input = inp\n",
    "\n",
    "        totals = np.dot(inp, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "\n",
    "        return np.exp(totals)/ np.sum(np.exp(totals), axis=0)\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the softmax layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        # We know only 1 element of d_L_d_out will be nonzero\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^totals\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "\n",
    "            # Sum of all e^totals\n",
    "            S = np.sum(t_exp)\n",
    "\n",
    "            # Gradients of out[i] against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.last_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "\n",
    "            # Gradients of loss against totals\n",
    "            d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "            # Gradients of loss against weights/biases/input\n",
    "            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_t_d_b\n",
    "            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "            # Update weights / biases\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.biases -= learn_rate * d_L_d_b\n",
    "\n",
    "            return d_L_d_inputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28238256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = X_train\n",
    "train_labels = y_train\n",
    "test_images = X_test\n",
    "test_labels = y_t\n",
    "f_loss = []\n",
    "f_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b08f12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN initialized!\n",
      "--- Epoch 1 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 3.217 | Accuracy: 0.170  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 3.245 | Accuracy: 0.160  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 3.240 | Accuracy: 0.220  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 3.239 | Accuracy: 0.170  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 3.242 | Accuracy: 0.190  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 3.231 | Accuracy: 0.230  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 3.225 | Accuracy: 0.300  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 3.226 | Accuracy: 0.210  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 3.225 | Accuracy: 0.200  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 3.230 | Accuracy: 0.190  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 3.221 | Accuracy: 0.220  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 3.210 | Accuracy: 0.190  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 3.208 | Accuracy: 0.270  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 3.212 | Accuracy: 0.240  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 3.182 | Accuracy: 0.340  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 3.199 | Accuracy: 0.200  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 3.178 | Accuracy: 0.270  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 3.181 | Accuracy: 0.260  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 3.162 | Accuracy: 0.170  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 3.165 | Accuracy: 0.270  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 3.181 | Accuracy: 0.190  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 3.163 | Accuracy: 0.170  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 3.177 | Accuracy: 0.250  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 3.150 | Accuracy: 0.270  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 3.175 | Accuracy: 0.140  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 3.168 | Accuracy: 0.210  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 3.123 | Accuracy: 0.220  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 3.211 | Accuracy: 0.140  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 3.146 | Accuracy: 0.210  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 3.175 | Accuracy: 0.170  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 3.146 | Accuracy: 0.260  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 3.176 | Accuracy: 0.210  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 3.102 | Accuracy: 0.250  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 3.143 | Accuracy: 0.220  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 3.128 | Accuracy: 0.220  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 3.137 | Accuracy: 0.220  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 3.092 | Accuracy: 0.270  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 3.085 | Accuracy: 0.300  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 3.160 | Accuracy: 0.230  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 3.091 | Accuracy: 0.240  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 3.153 | Accuracy: 0.160  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 3.139 | Accuracy: 0.190  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 3.129 | Accuracy: 0.230  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 3.166 | Accuracy: 0.250  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 3.076 | Accuracy: 0.250  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 3.096 | Accuracy: 0.240  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 3.119 | Accuracy: 0.250  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 3.075 | Accuracy: 0.270  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 3.172 | Accuracy: 0.170  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 3.067 | Accuracy: 0.230  | progress precent: 100.00\n",
      "---Epoch 1 ends---\n",
      "Average Loss of epoch : 3.894 | Accuracy : 0.266\n",
      "--- Epoch 2 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 3.038 | Accuracy: 0.310  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 3.009 | Accuracy: 0.280  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 3.114 | Accuracy: 0.240  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 3.050 | Accuracy: 0.230  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 3.118 | Accuracy: 0.140  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 3.039 | Accuracy: 0.310  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 3.059 | Accuracy: 0.210  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 3.057 | Accuracy: 0.260  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 3.072 | Accuracy: 0.270  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 3.030 | Accuracy: 0.300  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 3.001 | Accuracy: 0.260  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 3.073 | Accuracy: 0.200  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 3.109 | Accuracy: 0.210  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 3.048 | Accuracy: 0.220  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 3.154 | Accuracy: 0.190  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 2.979 | Accuracy: 0.340  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 3.108 | Accuracy: 0.190  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 3.087 | Accuracy: 0.250  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 3.045 | Accuracy: 0.210  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 3.068 | Accuracy: 0.200  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 3.027 | Accuracy: 0.230  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.975 | Accuracy: 0.260  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 3.002 | Accuracy: 0.230  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 3.074 | Accuracy: 0.240  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 3.033 | Accuracy: 0.240  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 3.091 | Accuracy: 0.230  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 3.159 | Accuracy: 0.160  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 3.001 | Accuracy: 0.250  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 3.032 | Accuracy: 0.250  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 3.011 | Accuracy: 0.230  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.966 | Accuracy: 0.320  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 3.143 | Accuracy: 0.190  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 3.115 | Accuracy: 0.210  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 3.038 | Accuracy: 0.220  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.994 | Accuracy: 0.260  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 3.014 | Accuracy: 0.260  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 3.078 | Accuracy: 0.220  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 3.058 | Accuracy: 0.290  | progress precent: 76.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3900] Past 100 steps: Average Loss 3.034 | Accuracy: 0.310  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.889 | Accuracy: 0.270  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.932 | Accuracy: 0.280  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 3.009 | Accuracy: 0.280  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 3.002 | Accuracy: 0.270  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 3.055 | Accuracy: 0.210  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 3.028 | Accuracy: 0.260  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 3.117 | Accuracy: 0.300  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 2.886 | Accuracy: 0.280  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 3.095 | Accuracy: 0.270  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 2.986 | Accuracy: 0.330  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.943 | Accuracy: 0.260  | progress precent: 100.00\n",
      "---Epoch 2 ends---\n",
      "Average Loss of epoch : 3.042 | Accuracy : 0.249\n",
      "--- Epoch 3 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.914 | Accuracy: 0.280  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 3.032 | Accuracy: 0.270  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 3.003 | Accuracy: 0.240  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 2.957 | Accuracy: 0.290  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.938 | Accuracy: 0.240  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 3.094 | Accuracy: 0.230  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 2.981 | Accuracy: 0.310  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 3.025 | Accuracy: 0.330  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.988 | Accuracy: 0.290  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 3.006 | Accuracy: 0.240  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.981 | Accuracy: 0.240  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 2.984 | Accuracy: 0.270  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 2.995 | Accuracy: 0.220  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.924 | Accuracy: 0.260  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 3.049 | Accuracy: 0.270  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 3.022 | Accuracy: 0.260  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 3.010 | Accuracy: 0.240  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.973 | Accuracy: 0.270  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 2.994 | Accuracy: 0.240  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 3.006 | Accuracy: 0.260  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 3.013 | Accuracy: 0.230  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.936 | Accuracy: 0.240  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.940 | Accuracy: 0.260  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 2.990 | Accuracy: 0.220  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.925 | Accuracy: 0.310  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 2.903 | Accuracy: 0.290  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 3.086 | Accuracy: 0.230  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.998 | Accuracy: 0.250  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 3.000 | Accuracy: 0.210  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 3.123 | Accuracy: 0.210  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.878 | Accuracy: 0.280  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 2.947 | Accuracy: 0.300  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.929 | Accuracy: 0.320  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 2.929 | Accuracy: 0.270  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.970 | Accuracy: 0.270  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 2.831 | Accuracy: 0.300  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 3.049 | Accuracy: 0.300  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.926 | Accuracy: 0.330  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 3.012 | Accuracy: 0.280  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 3.112 | Accuracy: 0.280  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 3.090 | Accuracy: 0.230  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 2.911 | Accuracy: 0.360  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.964 | Accuracy: 0.270  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 2.992 | Accuracy: 0.240  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.926 | Accuracy: 0.280  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 3.005 | Accuracy: 0.210  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 2.989 | Accuracy: 0.230  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 3.080 | Accuracy: 0.270  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 3.122 | Accuracy: 0.230  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.870 | Accuracy: 0.360  | progress precent: 100.00\n",
      "---Epoch 3 ends---\n",
      "Average Loss of epoch : 2.987 | Accuracy : 0.266\n",
      "--- Epoch 4 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.951 | Accuracy: 0.210  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 2.970 | Accuracy: 0.300  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 2.955 | Accuracy: 0.260  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 3.011 | Accuracy: 0.260  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.936 | Accuracy: 0.300  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 2.964 | Accuracy: 0.280  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 3.031 | Accuracy: 0.220  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 3.083 | Accuracy: 0.270  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.941 | Accuracy: 0.310  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 2.847 | Accuracy: 0.340  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.922 | Accuracy: 0.280  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 3.066 | Accuracy: 0.210  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 2.941 | Accuracy: 0.230  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.890 | Accuracy: 0.300  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 3.173 | Accuracy: 0.220  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 3.004 | Accuracy: 0.270  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 2.996 | Accuracy: 0.310  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.879 | Accuracy: 0.320  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 2.942 | Accuracy: 0.370  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 2.993 | Accuracy: 0.290  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 2.977 | Accuracy: 0.290  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.968 | Accuracy: 0.280  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.900 | Accuracy: 0.310  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 2.961 | Accuracy: 0.250  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.896 | Accuracy: 0.330  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 2.981 | Accuracy: 0.290  | progress precent: 52.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2700] Past 100 steps: Average Loss 3.083 | Accuracy: 0.230  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.937 | Accuracy: 0.320  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 2.938 | Accuracy: 0.280  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 2.852 | Accuracy: 0.350  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.905 | Accuracy: 0.300  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 3.001 | Accuracy: 0.260  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.777 | Accuracy: 0.420  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 3.033 | Accuracy: 0.260  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.954 | Accuracy: 0.280  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 2.993 | Accuracy: 0.290  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 2.839 | Accuracy: 0.320  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.806 | Accuracy: 0.390  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 2.939 | Accuracy: 0.280  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.834 | Accuracy: 0.350  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.888 | Accuracy: 0.330  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 2.945 | Accuracy: 0.290  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.992 | Accuracy: 0.280  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 3.100 | Accuracy: 0.260  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.921 | Accuracy: 0.380  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 2.959 | Accuracy: 0.250  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 2.862 | Accuracy: 0.290  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 2.825 | Accuracy: 0.300  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 2.904 | Accuracy: 0.290  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 3.092 | Accuracy: 0.260  | progress precent: 100.00\n",
      "---Epoch 4 ends---\n",
      "Average Loss of epoch : 2.952 | Accuracy : 0.291\n",
      "--- Epoch 5 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.954 | Accuracy: 0.260  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 2.877 | Accuracy: 0.310  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 2.901 | Accuracy: 0.280  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 2.972 | Accuracy: 0.300  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.863 | Accuracy: 0.340  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 2.973 | Accuracy: 0.330  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 2.944 | Accuracy: 0.340  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 2.966 | Accuracy: 0.230  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.731 | Accuracy: 0.360  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 2.949 | Accuracy: 0.340  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.763 | Accuracy: 0.330  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 2.925 | Accuracy: 0.290  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 2.749 | Accuracy: 0.350  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.888 | Accuracy: 0.390  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 3.051 | Accuracy: 0.250  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 2.930 | Accuracy: 0.360  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 3.038 | Accuracy: 0.260  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.837 | Accuracy: 0.300  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 3.033 | Accuracy: 0.280  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 2.942 | Accuracy: 0.280  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 2.946 | Accuracy: 0.290  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.962 | Accuracy: 0.240  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.929 | Accuracy: 0.290  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 3.005 | Accuracy: 0.260  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.898 | Accuracy: 0.360  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 3.041 | Accuracy: 0.290  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 2.997 | Accuracy: 0.250  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.964 | Accuracy: 0.230  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 2.934 | Accuracy: 0.320  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 2.850 | Accuracy: 0.270  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.867 | Accuracy: 0.330  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 2.876 | Accuracy: 0.330  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.920 | Accuracy: 0.350  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 2.884 | Accuracy: 0.300  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.963 | Accuracy: 0.300  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 2.821 | Accuracy: 0.310  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 2.956 | Accuracy: 0.290  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.994 | Accuracy: 0.300  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 2.835 | Accuracy: 0.380  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.994 | Accuracy: 0.250  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.917 | Accuracy: 0.280  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 2.793 | Accuracy: 0.300  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.823 | Accuracy: 0.290  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 2.940 | Accuracy: 0.320  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.984 | Accuracy: 0.340  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 2.886 | Accuracy: 0.310  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 3.070 | Accuracy: 0.230  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 2.858 | Accuracy: 0.290  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 3.016 | Accuracy: 0.240  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.960 | Accuracy: 0.320  | progress precent: 100.00\n",
      "---Epoch 5 ends---\n",
      "Average Loss of epoch : 2.924 | Accuracy : 0.301\n",
      "--- Epoch 6 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.881 | Accuracy: 0.310  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 2.825 | Accuracy: 0.390  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 2.967 | Accuracy: 0.240  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 2.979 | Accuracy: 0.270  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.837 | Accuracy: 0.330  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 2.826 | Accuracy: 0.410  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 2.970 | Accuracy: 0.250  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 2.947 | Accuracy: 0.290  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.957 | Accuracy: 0.280  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 2.926 | Accuracy: 0.310  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.725 | Accuracy: 0.400  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 2.900 | Accuracy: 0.350  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 3.086 | Accuracy: 0.260  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.886 | Accuracy: 0.350  | progress precent: 28.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1500] Past 100 steps: Average Loss 2.906 | Accuracy: 0.320  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 2.973 | Accuracy: 0.300  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 2.824 | Accuracy: 0.420  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.903 | Accuracy: 0.290  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 2.932 | Accuracy: 0.240  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 2.824 | Accuracy: 0.310  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 2.903 | Accuracy: 0.310  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.896 | Accuracy: 0.290  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.963 | Accuracy: 0.270  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 2.943 | Accuracy: 0.340  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.842 | Accuracy: 0.340  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 2.939 | Accuracy: 0.320  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 2.684 | Accuracy: 0.360  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.784 | Accuracy: 0.380  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 2.908 | Accuracy: 0.300  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 2.971 | Accuracy: 0.260  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.898 | Accuracy: 0.250  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 2.919 | Accuracy: 0.300  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.877 | Accuracy: 0.310  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 2.808 | Accuracy: 0.300  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.867 | Accuracy: 0.290  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 2.908 | Accuracy: 0.330  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 2.913 | Accuracy: 0.330  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.988 | Accuracy: 0.300  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 2.920 | Accuracy: 0.250  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.930 | Accuracy: 0.260  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.846 | Accuracy: 0.340  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 2.866 | Accuracy: 0.370  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.852 | Accuracy: 0.320  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 2.858 | Accuracy: 0.290  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.876 | Accuracy: 0.330  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 2.940 | Accuracy: 0.280  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 3.023 | Accuracy: 0.260  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 2.900 | Accuracy: 0.270  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 2.928 | Accuracy: 0.260  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.886 | Accuracy: 0.270  | progress precent: 100.00\n",
      "---Epoch 6 ends---\n",
      "Average Loss of epoch : 2.899 | Accuracy : 0.308\n",
      "--- Epoch 7 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.925 | Accuracy: 0.320  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 2.802 | Accuracy: 0.350  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 2.829 | Accuracy: 0.340  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 2.975 | Accuracy: 0.280  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.837 | Accuracy: 0.340  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 2.985 | Accuracy: 0.260  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 2.771 | Accuracy: 0.320  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 2.895 | Accuracy: 0.270  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.921 | Accuracy: 0.250  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 2.853 | Accuracy: 0.330  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.898 | Accuracy: 0.370  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 2.828 | Accuracy: 0.380  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 2.806 | Accuracy: 0.390  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.888 | Accuracy: 0.340  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 2.817 | Accuracy: 0.330  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 2.982 | Accuracy: 0.270  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 3.048 | Accuracy: 0.240  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.709 | Accuracy: 0.350  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 2.894 | Accuracy: 0.330  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 2.916 | Accuracy: 0.370  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 2.823 | Accuracy: 0.320  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.963 | Accuracy: 0.320  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.948 | Accuracy: 0.200  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 2.964 | Accuracy: 0.270  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.953 | Accuracy: 0.290  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 2.988 | Accuracy: 0.250  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 2.864 | Accuracy: 0.290  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.782 | Accuracy: 0.350  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 2.835 | Accuracy: 0.380  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 2.786 | Accuracy: 0.310  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.892 | Accuracy: 0.310  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 2.674 | Accuracy: 0.440  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.797 | Accuracy: 0.320  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 2.833 | Accuracy: 0.300  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.838 | Accuracy: 0.330  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 3.073 | Accuracy: 0.220  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 2.876 | Accuracy: 0.350  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.850 | Accuracy: 0.350  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 2.905 | Accuracy: 0.280  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.891 | Accuracy: 0.290  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.999 | Accuracy: 0.350  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 2.705 | Accuracy: 0.400  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.634 | Accuracy: 0.430  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 3.019 | Accuracy: 0.250  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.968 | Accuracy: 0.260  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 2.946 | Accuracy: 0.260  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 2.886 | Accuracy: 0.360  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 2.740 | Accuracy: 0.330  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 2.852 | Accuracy: 0.250  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.936 | Accuracy: 0.310  | progress precent: 100.00\n",
      "---Epoch 7 ends---\n",
      "Average Loss of epoch : 2.877 | Accuracy : 0.316\n",
      "--- Epoch 8 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.949 | Accuracy: 0.240  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 2.708 | Accuracy: 0.410  | progress precent: 4.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 300] Past 100 steps: Average Loss 3.135 | Accuracy: 0.190  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 2.804 | Accuracy: 0.350  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.961 | Accuracy: 0.230  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 2.815 | Accuracy: 0.300  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 2.689 | Accuracy: 0.380  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 2.778 | Accuracy: 0.390  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.819 | Accuracy: 0.360  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 2.777 | Accuracy: 0.360  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.862 | Accuracy: 0.380  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 2.679 | Accuracy: 0.390  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 2.859 | Accuracy: 0.270  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.715 | Accuracy: 0.390  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 2.922 | Accuracy: 0.270  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 2.941 | Accuracy: 0.250  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 2.890 | Accuracy: 0.320  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.864 | Accuracy: 0.280  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 2.942 | Accuracy: 0.220  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 2.786 | Accuracy: 0.330  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 2.792 | Accuracy: 0.350  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.884 | Accuracy: 0.390  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.830 | Accuracy: 0.340  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 2.823 | Accuracy: 0.370  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.882 | Accuracy: 0.310  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 2.738 | Accuracy: 0.350  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 2.766 | Accuracy: 0.400  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.832 | Accuracy: 0.340  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 2.879 | Accuracy: 0.260  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 2.906 | Accuracy: 0.320  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.933 | Accuracy: 0.370  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 2.743 | Accuracy: 0.430  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.901 | Accuracy: 0.260  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 2.917 | Accuracy: 0.370  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 3.053 | Accuracy: 0.280  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 2.773 | Accuracy: 0.410  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 3.028 | Accuracy: 0.240  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.844 | Accuracy: 0.290  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 2.968 | Accuracy: 0.280  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.752 | Accuracy: 0.340  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.898 | Accuracy: 0.260  | progress precent: 82.00\n",
      "[Step 4200] Past 100 steps: Average Loss 2.915 | Accuracy: 0.320  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.763 | Accuracy: 0.380  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 2.853 | Accuracy: 0.280  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.932 | Accuracy: 0.310  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 2.768 | Accuracy: 0.340  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 2.862 | Accuracy: 0.310  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 2.895 | Accuracy: 0.320  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 2.857 | Accuracy: 0.280  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.864 | Accuracy: 0.330  | progress precent: 100.00\n",
      "---Epoch 8 ends---\n",
      "Average Loss of epoch : 2.856 | Accuracy : 0.323\n",
      "--- Epoch 9 begins---\n",
      "[Step 100] Past 100 steps: Average Loss 2.774 | Accuracy: 0.330  | progress precent: 2.00\n",
      "[Step 200] Past 100 steps: Average Loss 3.036 | Accuracy: 0.270  | progress precent: 4.00\n",
      "[Step 300] Past 100 steps: Average Loss 2.913 | Accuracy: 0.230  | progress precent: 6.00\n",
      "[Step 400] Past 100 steps: Average Loss 2.877 | Accuracy: 0.370  | progress precent: 8.00\n",
      "[Step 500] Past 100 steps: Average Loss 2.720 | Accuracy: 0.360  | progress precent: 10.00\n",
      "[Step 600] Past 100 steps: Average Loss 2.827 | Accuracy: 0.410  | progress precent: 12.00\n",
      "[Step 700] Past 100 steps: Average Loss 2.783 | Accuracy: 0.310  | progress precent: 14.00\n",
      "[Step 800] Past 100 steps: Average Loss 2.851 | Accuracy: 0.320  | progress precent: 16.00\n",
      "[Step 900] Past 100 steps: Average Loss 2.843 | Accuracy: 0.320  | progress precent: 18.00\n",
      "[Step 1000] Past 100 steps: Average Loss 2.873 | Accuracy: 0.280  | progress precent: 20.00\n",
      "[Step 1100] Past 100 steps: Average Loss 2.812 | Accuracy: 0.340  | progress precent: 22.00\n",
      "[Step 1200] Past 100 steps: Average Loss 2.818 | Accuracy: 0.320  | progress precent: 24.00\n",
      "[Step 1300] Past 100 steps: Average Loss 2.787 | Accuracy: 0.350  | progress precent: 26.00\n",
      "[Step 1400] Past 100 steps: Average Loss 2.828 | Accuracy: 0.320  | progress precent: 28.00\n",
      "[Step 1500] Past 100 steps: Average Loss 2.724 | Accuracy: 0.430  | progress precent: 30.00\n",
      "[Step 1600] Past 100 steps: Average Loss 2.939 | Accuracy: 0.260  | progress precent: 32.00\n",
      "[Step 1700] Past 100 steps: Average Loss 2.886 | Accuracy: 0.330  | progress precent: 34.00\n",
      "[Step 1800] Past 100 steps: Average Loss 2.691 | Accuracy: 0.400  | progress precent: 36.00\n",
      "[Step 1900] Past 100 steps: Average Loss 2.790 | Accuracy: 0.380  | progress precent: 38.00\n",
      "[Step 2000] Past 100 steps: Average Loss 2.848 | Accuracy: 0.320  | progress precent: 40.00\n",
      "[Step 2100] Past 100 steps: Average Loss 2.853 | Accuracy: 0.330  | progress precent: 42.00\n",
      "[Step 2200] Past 100 steps: Average Loss 2.886 | Accuracy: 0.330  | progress precent: 44.00\n",
      "[Step 2300] Past 100 steps: Average Loss 2.988 | Accuracy: 0.260  | progress precent: 46.00\n",
      "[Step 2400] Past 100 steps: Average Loss 2.837 | Accuracy: 0.330  | progress precent: 48.00\n",
      "[Step 2500] Past 100 steps: Average Loss 2.762 | Accuracy: 0.350  | progress precent: 50.00\n",
      "[Step 2600] Past 100 steps: Average Loss 2.875 | Accuracy: 0.290  | progress precent: 52.00\n",
      "[Step 2700] Past 100 steps: Average Loss 2.852 | Accuracy: 0.350  | progress precent: 54.00\n",
      "[Step 2800] Past 100 steps: Average Loss 2.974 | Accuracy: 0.240  | progress precent: 56.00\n",
      "[Step 2900] Past 100 steps: Average Loss 2.828 | Accuracy: 0.320  | progress precent: 58.00\n",
      "[Step 3000] Past 100 steps: Average Loss 2.877 | Accuracy: 0.310  | progress precent: 60.00\n",
      "[Step 3100] Past 100 steps: Average Loss 2.826 | Accuracy: 0.460  | progress precent: 62.00\n",
      "[Step 3200] Past 100 steps: Average Loss 2.749 | Accuracy: 0.380  | progress precent: 64.00\n",
      "[Step 3300] Past 100 steps: Average Loss 2.911 | Accuracy: 0.330  | progress precent: 66.00\n",
      "[Step 3400] Past 100 steps: Average Loss 2.780 | Accuracy: 0.350  | progress precent: 68.00\n",
      "[Step 3500] Past 100 steps: Average Loss 2.738 | Accuracy: 0.350  | progress precent: 70.00\n",
      "[Step 3600] Past 100 steps: Average Loss 2.829 | Accuracy: 0.300  | progress precent: 72.00\n",
      "[Step 3700] Past 100 steps: Average Loss 2.908 | Accuracy: 0.330  | progress precent: 74.00\n",
      "[Step 3800] Past 100 steps: Average Loss 2.908 | Accuracy: 0.360  | progress precent: 76.00\n",
      "[Step 3900] Past 100 steps: Average Loss 2.630 | Accuracy: 0.430  | progress precent: 78.00\n",
      "[Step 4000] Past 100 steps: Average Loss 2.845 | Accuracy: 0.350  | progress precent: 80.00\n",
      "[Step 4100] Past 100 steps: Average Loss 2.750 | Accuracy: 0.390  | progress precent: 82.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4200] Past 100 steps: Average Loss 2.771 | Accuracy: 0.390  | progress precent: 84.00\n",
      "[Step 4300] Past 100 steps: Average Loss 2.742 | Accuracy: 0.410  | progress precent: 86.00\n",
      "[Step 4400] Past 100 steps: Average Loss 2.801 | Accuracy: 0.410  | progress precent: 88.00\n",
      "[Step 4500] Past 100 steps: Average Loss 2.855 | Accuracy: 0.310  | progress precent: 90.00\n",
      "[Step 4600] Past 100 steps: Average Loss 2.848 | Accuracy: 0.350  | progress precent: 92.00\n",
      "[Step 4700] Past 100 steps: Average Loss 2.928 | Accuracy: 0.310  | progress precent: 94.00\n",
      "[Step 4800] Past 100 steps: Average Loss 2.747 | Accuracy: 0.390  | progress precent: 96.00\n",
      "[Step 4900] Past 100 steps: Average Loss 2.861 | Accuracy: 0.290  | progress precent: 98.00\n",
      "[Step 5000] Past 100 steps: Average Loss 2.854 | Accuracy: 0.260  | progress precent: 100.00\n",
      "---Epoch 9 ends---\n",
      "Average Loss of epoch : 2.834 | Accuracy : 0.338\n"
     ]
    }
   ],
   "source": [
    "conv = Conv(num_filters=16)                  # 32x32x1 -> 28x28x16\n",
    "pool = MaxPool2()                   # 28x28x16 -> 14x14x16\n",
    "\n",
    "softmax = Softmax(14 * 14 * 16, 10) # 15x15x16 -> 10\n",
    "\n",
    "def forward(image, label):\n",
    "    '''\n",
    "    Completes a forward pass of the CNN and calculates the accuracy and\n",
    "    cross-entropy loss.\n",
    "    - image is a 2d numpy array\n",
    "    - label is a digit\n",
    "    '''\n",
    "    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "    # to work with. This is standard practice.\n",
    "    out = conv.forward((image / 255) - 0.5)\n",
    "    out = pool.forward(out)\n",
    "    out = softmax.forward(out)\n",
    "    \n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
    "    for i in range(len(out)):\n",
    "        if i!=label:\n",
    "            loss -= np.log(1-out[i])\n",
    "        else:\n",
    "            loss -= np.log(out[label])\n",
    "    \n",
    "    acc = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "    return out, loss, acc\n",
    "\n",
    "def train(im, label, lr=.0005):\n",
    "    '''\n",
    "    Completes a full training step on the given image and label.\n",
    "    Returns the cross-entropy loss and accuracy.\n",
    "    - image is a 2d numpy array\n",
    "    - label is a digit\n",
    "    - lr is the learning rate\n",
    "    '''\n",
    "    # Forward\n",
    "    out, loss, acc = forward(im, label)\n",
    "\n",
    "    # Calculate initial gradient\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1 / out[label]\n",
    "\n",
    "    # Backprop\n",
    "    gradient = softmax.backprop(gradient, lr)\n",
    "    gradient = pool.backprop(gradient)\n",
    "    gradient = conv.backprop(gradient, lr)\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "print('MNIST CNN initialized!')\n",
    "\n",
    "\n",
    "# Train the CNN for 3 epochs\n",
    "for epoch in range(8):\n",
    "    print('--- Epoch %d begins---' % (epoch + 1))\n",
    "\n",
    "    # Shuffle the training data\n",
    "    permutation = np.random.permutation(len(train_images))\n",
    "    train_images = train_images[permutation]\n",
    "    train_labels = train_labels[permutation]\n",
    "    train_images = train_images[0:5000]\n",
    "    train_labels = train_labels[0:5000]\n",
    "\n",
    "    # Train!\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    f_loss.append(loss)\n",
    "    f_acc.append(num_correct)\n",
    "    \n",
    "    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "        if i % 100 == 99:\n",
    "            print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %.3f  | progress precent: %.2f' %\n",
    "                (i + 1, loss / 100, num_correct/100,((i+1)/len(train_images))*100))\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "\n",
    "        l, acc = train(im, label)\n",
    "        loss += l\n",
    "        f_loss[epoch] += l\n",
    "        num_correct += acc\n",
    "        f_acc[epoch] += acc\n",
    "    print(\"---Epoch %d ends---\"% (epoch + 1))\n",
    "    print('Average Loss of epoch : %.3f | Accuracy : %.3f'% (f_loss[epoch]/5000,f_acc[epoch]/5000))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98cd2c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.89393033, 3.04151252, 2.98705165, 2.95178226, 2.92404654,\n",
       "       2.89872691, 2.87653066, 2.85564149])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_loss = np.array(f_loss)[0:8]\n",
    "f_acc = np.array(f_acc)[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28f894da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,9,1),f_loss,label='loss',marker =\"*\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"loss.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d47b62cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,9,1),f_acc, label = 'accuracy',marker =\"o\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"accuracy.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2582ae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the CNN ---\n",
      "Test Loss: 2.8873072249543372\n",
      "Test Accuracy: 0.319\n"
     ]
    }
   ],
   "source": [
    "# Test the CNN\n",
    "\n",
    "print('\\n--- Testing the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for i ,(im, label) in enumerate(zip(test_images[0:1000], test_labels[0:1000])):\n",
    "    _, l, acc = forward(im, label)\n",
    "    loss += l\n",
    "    num_correct += acc\n",
    "\n",
    "num_tests = len(test_images[0:1000])\n",
    "print('Test Loss:', loss / num_tests)\n",
    "print('Test Accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49360cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
